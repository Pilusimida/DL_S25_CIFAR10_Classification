{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!pip install -q kaggle\n",
        "!kaggle --version\n",
        "!kaggle competitions download -c deep-learning-spring-2025-project-1\n",
        "!unzip -q deep-learning-spring-2025-project-1.zip -d ./data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "rIR2vKla6pRW",
        "outputId": "16611cf1-abf3-4f71-f2d4-529ad341e6d4"
      },
      "id": "rIR2vKla6pRW",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-23f36772-c8e3-4f65-8789-3ca9c4644d6e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-23f36772-c8e3-4f65-8789-3ca9c4644d6e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving data_handler.py to data_handler (1).py\n",
            "mv: cannot stat 'kaggle.json': No such file or directory\n",
            "Kaggle API 1.6.17\n",
            "deep-learning-spring-2025-project-1.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "replace ./data/cifar-10-python/cifar-10-batches-py/batches.meta? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "\n",
            "n\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "847113d0cd4034e6"
      },
      "cell_type": "markdown",
      "source": [
        "1. Import dependency"
      ],
      "id": "847113d0cd4034e6"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-13T07:20:56.823126Z",
          "start_time": "2025-03-13T07:20:56.819692Z"
        },
        "id": "efb1f80b5f47bedd"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from data_handler import DataHandler\n",
        "import numpy as np\n",
        "import torch.nn.functional as F"
      ],
      "id": "efb1f80b5f47bedd",
      "outputs": [],
      "execution_count": 2
    },
    {
      "metadata": {
        "id": "2cd4077e329f3fed"
      },
      "cell_type": "markdown",
      "source": [
        " 2. Dataset"
      ],
      "id": "2cd4077e329f3fed"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-13T07:20:57.698109Z",
          "start_time": "2025-03-13T07:20:57.694306Z"
        },
        "id": "8aa1c00c212a35a"
      },
      "cell_type": "code",
      "source": [
        "# 数据集类\n",
        "class CIFAR10Dataset(Dataset):\n",
        "    def __init__(self, data_handler, files, transform=None):\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        self.transform = transform\n",
        "\n",
        "        for file in files:\n",
        "            batch = data_handler.unpickle(file)\n",
        "            images = batch[b\"data\"].reshape(-1, 3, 32, 32).astype(np.uint8)\n",
        "            labels = batch[b\"labels\"]\n",
        "            self.data.append(images)\n",
        "            self.labels.extend(labels)\n",
        "\n",
        "        self.data = np.vstack(self.data)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.data[idx]\n",
        "        image = np.transpose(image, (1, 2, 0))\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n"
      ],
      "id": "8aa1c00c212a35a",
      "outputs": [],
      "execution_count": 3
    },
    {
      "metadata": {
        "id": "7e7b466896e08482"
      },
      "cell_type": "markdown",
      "source": [
        "3. Data Process"
      ],
      "id": "7e7b466896e08482"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-13T07:20:59.029565Z",
          "start_time": "2025-03-13T07:20:58.718722Z"
        },
        "id": "be0c5fe06f320626"
      },
      "cell_type": "code",
      "source": [
        "# 数据处理\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "])\n",
        "\n",
        "data_handler = DataHandler(\"./data/cifar-10-python/cifar-10-batches-py/\")\n",
        "train_files = [f\"data_batch_{i}\" for i in range(1, 6)]\n",
        "test_files = [\"test_batch\"]\n",
        "\n",
        "train_dataset = CIFAR10Dataset(data_handler, train_files, transform=transform)\n",
        "test_dataset = CIFAR10Dataset(data_handler, test_files, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n"
      ],
      "id": "be0c5fe06f320626",
      "outputs": [],
      "execution_count": 4
    },
    {
      "metadata": {
        "id": "efbdf0eb1c0dd9d8"
      },
      "cell_type": "markdown",
      "source": [
        "4. Build Model"
      ],
      "id": "efbdf0eb1c0dd9d8"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-13T07:20:59.808676Z",
          "start_time": "2025-03-13T07:20:59.802425Z"
        },
        "id": "c8bde7ad18aa3c0b"
      },
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, stride=1):\n",
        "    super(ResidualBlock, self).__init__()\n",
        "    # Using 3x3 square kernel, padding=1 ensure that the output size is the same as the input size.\n",
        "    self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=True)\n",
        "    self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True)\n",
        "    self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    self.sample = None\n",
        "    if stride != 1 or in_channels != out_channels:\n",
        "      self.sample = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=True),\n",
        "        nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "  def forward(self, x):\n",
        "    residualx = x\n",
        "    if self.sample is not None:\n",
        "      residualx = self.sample(x)\n",
        "\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = nn.ReLU(inplace=True)(out)\n",
        "\n",
        "    out = self.conv2(out)\n",
        "    out = self.bn2(out)\n",
        "    out += residualx\n",
        "    out = nn.ReLU(inplace=True)(out)\n",
        "\n",
        "    return out"
      ],
      "id": "c8bde7ad18aa3c0b",
      "outputs": [],
      "execution_count": 5
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-13T08:00:09.186720Z",
          "start_time": "2025-03-13T08:00:09.176693Z"
        },
        "id": "6b6de078e776ac27"
      },
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "  def __init__(self, block, layers, num_classes = 10):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.in_channels = 64\n",
        "    self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(64)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    self.layer1 = self._make_layer(block, 50, layers[0])\n",
        "    self.layer2 = self._make_layer(block, 100, layers[1], stride=2)\n",
        "    self.layer3 = self._make_layer(block, 200, layers[2], stride=2)\n",
        "    # self.layer4 = self._make_layer(block, 336, layers[3], stride=2)\n",
        "    self.layer4 = self._make_layer(block, 200, layers[3], stride=2)\n",
        "    self.layer5 = self._make_layer(block, 200, layers[3], stride=2)\n",
        "\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "    # self.fc = nn.Linear(336, num_classes)\n",
        "    self.fc_1 = nn.Linear(200, 100)\n",
        "    self.fc_2 = nn.Linear(100, 50)\n",
        "    self.fc_3 = nn.Linear(50, num_classes)\n",
        "\n",
        "\n",
        "  def _make_layer(self, block, out_channels, num_blocks, stride=1):\n",
        "    strides = [stride] + [1] * (num_blocks - 1)\n",
        "    layers = []\n",
        "    for stride in strides:\n",
        "      layers.append(block(self.in_channels, out_channels, stride))\n",
        "      self.in_channels = out_channels\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    # x = self.maxpool(x)\n",
        "\n",
        "    x = self.layer1(x)\n",
        "    x = self.layer2(x)\n",
        "    x = self.layer3(x)\n",
        "    x = self.layer4(x)\n",
        "\n",
        "    x = self.layer5(x)\n",
        "\n",
        "    x = self.avgpool(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    # x = self.fc(x)\n",
        "    x = self.relu(self.fc_1(x))\n",
        "    x = self.relu(self.fc_2(x))\n",
        "    x = self.relu(self.fc_3(x))\n",
        "    x = F.log_softmax(x, dim=1)\n",
        "\n",
        "    return x"
      ],
      "id": "6b6de078e776ac27",
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(model, train_loader, test_loader, optimizer, criterion, num_epochs=10):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "            total_train += labels.size(0)\n",
        "\n",
        "        train_accuracy = 100 * correct_train / total_train\n",
        "\n",
        "        # === 测试阶段 ===\n",
        "        model.eval()\n",
        "        correct_test = 0\n",
        "        total_test = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                correct_test += (predicted == labels).sum().item()\n",
        "                total_test += labels.size(0)\n",
        "\n",
        "        test_accuracy = 100 * correct_test / total_test\n",
        "        print(f\"Epoch{epoch},train_accuracy:{train_accuracy},test_accuracy:{test_accuracy}\")\n",
        "    return train_accuracy, test_accuracy"
      ],
      "metadata": {
        "id": "ox-PP8s4oGMc"
      },
      "id": "ox-PP8s4oGMc",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layers = [2, 2, 2, 2]\n",
        "model = ResNet(ResidualBlock, layers).cuda()\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "from torchsummary import summary\n",
        "summary(model, input_size=(3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMrHyCinplMp",
        "outputId": "d404db0d-b2b2-4239-9f07-30dc2ec546e3"
      },
      "id": "OMrHyCinplMp",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "              ReLU-3           [-1, 64, 32, 32]               0\n",
            "            Conv2d-4           [-1, 50, 32, 32]           3,250\n",
            "       BatchNorm2d-5           [-1, 50, 32, 32]             100\n",
            "            Conv2d-6           [-1, 50, 32, 32]          28,850\n",
            "       BatchNorm2d-7           [-1, 50, 32, 32]             100\n",
            "            Conv2d-8           [-1, 50, 32, 32]          22,550\n",
            "       BatchNorm2d-9           [-1, 50, 32, 32]             100\n",
            "    ResidualBlock-10           [-1, 50, 32, 32]               0\n",
            "           Conv2d-11           [-1, 50, 32, 32]          22,550\n",
            "      BatchNorm2d-12           [-1, 50, 32, 32]             100\n",
            "           Conv2d-13           [-1, 50, 32, 32]          22,550\n",
            "      BatchNorm2d-14           [-1, 50, 32, 32]             100\n",
            "    ResidualBlock-15           [-1, 50, 32, 32]               0\n",
            "           Conv2d-16          [-1, 100, 16, 16]           5,100\n",
            "      BatchNorm2d-17          [-1, 100, 16, 16]             200\n",
            "           Conv2d-18          [-1, 100, 16, 16]          45,100\n",
            "      BatchNorm2d-19          [-1, 100, 16, 16]             200\n",
            "           Conv2d-20          [-1, 100, 16, 16]          90,100\n",
            "      BatchNorm2d-21          [-1, 100, 16, 16]             200\n",
            "    ResidualBlock-22          [-1, 100, 16, 16]               0\n",
            "           Conv2d-23          [-1, 100, 16, 16]          90,100\n",
            "      BatchNorm2d-24          [-1, 100, 16, 16]             200\n",
            "           Conv2d-25          [-1, 100, 16, 16]          90,100\n",
            "      BatchNorm2d-26          [-1, 100, 16, 16]             200\n",
            "    ResidualBlock-27          [-1, 100, 16, 16]               0\n",
            "           Conv2d-28            [-1, 200, 8, 8]          20,200\n",
            "      BatchNorm2d-29            [-1, 200, 8, 8]             400\n",
            "           Conv2d-30            [-1, 200, 8, 8]         180,200\n",
            "      BatchNorm2d-31            [-1, 200, 8, 8]             400\n",
            "           Conv2d-32            [-1, 200, 8, 8]         360,200\n",
            "      BatchNorm2d-33            [-1, 200, 8, 8]             400\n",
            "    ResidualBlock-34            [-1, 200, 8, 8]               0\n",
            "           Conv2d-35            [-1, 200, 8, 8]         360,200\n",
            "      BatchNorm2d-36            [-1, 200, 8, 8]             400\n",
            "           Conv2d-37            [-1, 200, 8, 8]         360,200\n",
            "      BatchNorm2d-38            [-1, 200, 8, 8]             400\n",
            "    ResidualBlock-39            [-1, 200, 8, 8]               0\n",
            "           Conv2d-40            [-1, 200, 4, 4]          40,200\n",
            "      BatchNorm2d-41            [-1, 200, 4, 4]             400\n",
            "           Conv2d-42            [-1, 200, 4, 4]         360,200\n",
            "      BatchNorm2d-43            [-1, 200, 4, 4]             400\n",
            "           Conv2d-44            [-1, 200, 4, 4]         360,200\n",
            "      BatchNorm2d-45            [-1, 200, 4, 4]             400\n",
            "    ResidualBlock-46            [-1, 200, 4, 4]               0\n",
            "           Conv2d-47            [-1, 200, 4, 4]         360,200\n",
            "      BatchNorm2d-48            [-1, 200, 4, 4]             400\n",
            "           Conv2d-49            [-1, 200, 4, 4]         360,200\n",
            "      BatchNorm2d-50            [-1, 200, 4, 4]             400\n",
            "    ResidualBlock-51            [-1, 200, 4, 4]               0\n",
            "           Conv2d-52            [-1, 200, 2, 2]          40,200\n",
            "      BatchNorm2d-53            [-1, 200, 2, 2]             400\n",
            "           Conv2d-54            [-1, 200, 2, 2]         360,200\n",
            "      BatchNorm2d-55            [-1, 200, 2, 2]             400\n",
            "           Conv2d-56            [-1, 200, 2, 2]         360,200\n",
            "      BatchNorm2d-57            [-1, 200, 2, 2]             400\n",
            "    ResidualBlock-58            [-1, 200, 2, 2]               0\n",
            "           Conv2d-59            [-1, 200, 2, 2]         360,200\n",
            "      BatchNorm2d-60            [-1, 200, 2, 2]             400\n",
            "           Conv2d-61            [-1, 200, 2, 2]         360,200\n",
            "      BatchNorm2d-62            [-1, 200, 2, 2]             400\n",
            "    ResidualBlock-63            [-1, 200, 2, 2]               0\n",
            "AdaptiveAvgPool2d-64            [-1, 200, 1, 1]               0\n",
            "           Linear-65                  [-1, 100]          20,100\n",
            "             ReLU-66                  [-1, 100]               0\n",
            "           Linear-67                   [-1, 50]           5,050\n",
            "             ReLU-68                   [-1, 50]               0\n",
            "           Linear-69                   [-1, 10]             510\n",
            "             ReLU-70                   [-1, 10]               0\n",
            "================================================================\n",
            "Total params: 4,698,266\n",
            "Trainable params: 4,698,266\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 10.07\n",
            "Params size (MB): 17.92\n",
            "Estimated Total Size (MB): 28.01\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-13T08:00:11.148840Z",
          "start_time": "2025-03-13T08:00:11.106453Z"
        },
        "id": "d00674217f036b38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91a2e5b1-fe0a-45f7-84a6-d3d7e26cf4d4"
      },
      "cell_type": "code",
      "source": [
        "learning_rates = [0.005, 0.001, 0.0005]\n",
        "results = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    model = ResNet(ResidualBlock, layers).cuda()\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "    print(f\"\\nTraining with SGD (lr={lr})...\")\n",
        "    train_acc, test_acc = train_and_evaluate(model, train_loader, test_loader, optimizer, criterion, 50)\n",
        "\n",
        "    results.append({\n",
        "        'optimizer': optimizer,\n",
        "        'learning_rate': lr,\n",
        "        'train_acc': train_acc,\n",
        "        'test_acc': test_acc\n",
        "    })"
      ],
      "id": "d00674217f036b38",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with SGD (lr=0.005)...\n",
            "Epoch0,train_accuracy:31.744,test_accuracy:43.11\n",
            "Epoch1,train_accuracy:54.968,test_accuracy:60.64\n",
            "Epoch2,train_accuracy:66.212,test_accuracy:66.58\n",
            "Epoch3,train_accuracy:72.238,test_accuracy:68.95\n",
            "Epoch4,train_accuracy:76.766,test_accuracy:76.82\n",
            "Epoch5,train_accuracy:79.228,test_accuracy:80.0\n",
            "Epoch6,train_accuracy:81.738,test_accuracy:80.09\n",
            "Epoch7,train_accuracy:83.222,test_accuracy:80.03\n",
            "Epoch8,train_accuracy:84.566,test_accuracy:80.03\n",
            "Epoch9,train_accuracy:85.53,test_accuracy:79.58\n",
            "Epoch10,train_accuracy:86.744,test_accuracy:83.15\n",
            "Epoch11,train_accuracy:87.53,test_accuracy:85.19\n",
            "Epoch12,train_accuracy:88.298,test_accuracy:84.47\n",
            "Epoch13,train_accuracy:88.898,test_accuracy:82.32\n",
            "Epoch14,train_accuracy:89.548,test_accuracy:85.23\n",
            "Epoch15,train_accuracy:90.18,test_accuracy:86.87\n",
            "Epoch16,train_accuracy:90.594,test_accuracy:86.68\n",
            "Epoch17,train_accuracy:91.234,test_accuracy:86.58\n",
            "Epoch18,train_accuracy:91.396,test_accuracy:85.41\n",
            "Epoch19,train_accuracy:91.77,test_accuracy:86.79\n",
            "Epoch20,train_accuracy:92.428,test_accuracy:87.04\n",
            "Epoch21,train_accuracy:92.66,test_accuracy:87.74\n",
            "Epoch22,train_accuracy:92.846,test_accuracy:88.54\n",
            "Epoch23,train_accuracy:93.372,test_accuracy:88.63\n",
            "Epoch24,train_accuracy:93.688,test_accuracy:88.18\n",
            "Epoch25,train_accuracy:94.062,test_accuracy:88.59\n",
            "Epoch26,train_accuracy:94.104,test_accuracy:86.04\n",
            "Epoch27,train_accuracy:94.37,test_accuracy:87.86\n",
            "Epoch28,train_accuracy:94.64,test_accuracy:87.67\n",
            "Epoch29,train_accuracy:94.836,test_accuracy:88.07\n",
            "Epoch30,train_accuracy:94.964,test_accuracy:88.87\n",
            "Epoch31,train_accuracy:95.23,test_accuracy:88.32\n",
            "Epoch32,train_accuracy:95.374,test_accuracy:89.25\n",
            "Epoch33,train_accuracy:95.558,test_accuracy:87.97\n",
            "Epoch34,train_accuracy:95.818,test_accuracy:88.66\n",
            "Epoch35,train_accuracy:95.904,test_accuracy:90.19\n",
            "Epoch36,train_accuracy:96.078,test_accuracy:89.26\n",
            "Epoch37,train_accuracy:96.232,test_accuracy:89.39\n",
            "Epoch38,train_accuracy:96.186,test_accuracy:89.53\n",
            "Epoch39,train_accuracy:96.568,test_accuracy:89.05\n",
            "Epoch40,train_accuracy:96.664,test_accuracy:88.92\n",
            "Epoch41,train_accuracy:96.842,test_accuracy:89.74\n",
            "Epoch42,train_accuracy:96.922,test_accuracy:89.96\n",
            "Epoch43,train_accuracy:97.022,test_accuracy:89.1\n",
            "Epoch44,train_accuracy:97.116,test_accuracy:88.74\n",
            "Epoch45,train_accuracy:97.106,test_accuracy:89.64\n",
            "Epoch46,train_accuracy:97.29,test_accuracy:89.17\n",
            "Epoch47,train_accuracy:97.334,test_accuracy:90.11\n",
            "Epoch48,train_accuracy:97.568,test_accuracy:89.72\n",
            "Epoch49,train_accuracy:97.442,test_accuracy:89.13\n",
            "\n",
            "Training with SGD (lr=0.001)...\n",
            "Epoch0,train_accuracy:16.622,test_accuracy:22.75\n",
            "Epoch1,train_accuracy:29.914,test_accuracy:36.14\n",
            "Epoch2,train_accuracy:38.52,test_accuracy:41.45\n",
            "Epoch3,train_accuracy:45.102,test_accuracy:46.6\n",
            "Epoch4,train_accuracy:49.574,test_accuracy:51.63\n",
            "Epoch5,train_accuracy:52.96,test_accuracy:53.46\n",
            "Epoch6,train_accuracy:56.768,test_accuracy:58.39\n",
            "Epoch7,train_accuracy:62.81,test_accuracy:62.63\n",
            "Epoch8,train_accuracy:65.642,test_accuracy:63.79\n",
            "Epoch9,train_accuracy:67.514,test_accuracy:66.55\n",
            "Epoch10,train_accuracy:69.462,test_accuracy:67.34\n",
            "Epoch11,train_accuracy:70.892,test_accuracy:68.94\n",
            "Epoch12,train_accuracy:72.106,test_accuracy:70.73\n",
            "Epoch13,train_accuracy:73.168,test_accuracy:69.49\n",
            "Epoch14,train_accuracy:75.382,test_accuracy:79.05\n",
            "Epoch15,train_accuracy:82.2,test_accuracy:80.09\n",
            "Epoch16,train_accuracy:83.482,test_accuracy:81.18\n",
            "Epoch17,train_accuracy:84.144,test_accuracy:81.52\n",
            "Epoch18,train_accuracy:84.808,test_accuracy:82.07\n",
            "Epoch19,train_accuracy:85.65,test_accuracy:81.44\n",
            "Epoch20,train_accuracy:86.228,test_accuracy:83.29\n",
            "Epoch21,train_accuracy:86.996,test_accuracy:83.59\n",
            "Epoch22,train_accuracy:87.352,test_accuracy:83.52\n",
            "Epoch23,train_accuracy:87.812,test_accuracy:83.54\n",
            "Epoch24,train_accuracy:88.342,test_accuracy:83.58\n",
            "Epoch25,train_accuracy:88.996,test_accuracy:83.34\n",
            "Epoch26,train_accuracy:89.332,test_accuracy:84.0\n",
            "Epoch27,train_accuracy:89.564,test_accuracy:84.92\n",
            "Epoch28,train_accuracy:90.036,test_accuracy:84.81\n",
            "Epoch29,train_accuracy:90.454,test_accuracy:84.41\n",
            "Epoch30,train_accuracy:90.738,test_accuracy:85.33\n",
            "Epoch31,train_accuracy:91.052,test_accuracy:85.1\n",
            "Epoch32,train_accuracy:91.398,test_accuracy:85.87\n",
            "Epoch33,train_accuracy:91.68,test_accuracy:85.65\n",
            "Epoch34,train_accuracy:91.904,test_accuracy:85.83\n",
            "Epoch35,train_accuracy:92.18,test_accuracy:86.07\n",
            "Epoch36,train_accuracy:92.544,test_accuracy:86.77\n",
            "Epoch37,train_accuracy:92.882,test_accuracy:85.74\n",
            "Epoch38,train_accuracy:93.218,test_accuracy:86.16\n",
            "Epoch39,train_accuracy:93.106,test_accuracy:86.61\n",
            "Epoch40,train_accuracy:93.378,test_accuracy:87.01\n",
            "Epoch41,train_accuracy:93.524,test_accuracy:86.08\n",
            "Epoch42,train_accuracy:93.714,test_accuracy:87.79\n",
            "Epoch43,train_accuracy:93.99,test_accuracy:86.82\n",
            "Epoch44,train_accuracy:94.306,test_accuracy:86.63\n",
            "Epoch45,train_accuracy:94.586,test_accuracy:86.23\n",
            "Epoch46,train_accuracy:94.728,test_accuracy:86.81\n",
            "Epoch47,train_accuracy:94.752,test_accuracy:87.3\n",
            "Epoch48,train_accuracy:94.924,test_accuracy:87.58\n",
            "Epoch49,train_accuracy:95.234,test_accuracy:86.69\n",
            "\n",
            "Training with SGD (lr=0.0005)...\n",
            "Epoch0,train_accuracy:13.322,test_accuracy:17.32\n",
            "Epoch1,train_accuracy:18.544,test_accuracy:19.61\n",
            "Epoch2,train_accuracy:21.54,test_accuracy:25.67\n",
            "Epoch3,train_accuracy:27.82,test_accuracy:30.49\n",
            "Epoch4,train_accuracy:34.212,test_accuracy:36.96\n",
            "Epoch5,train_accuracy:38.808,test_accuracy:40.1\n",
            "Epoch6,train_accuracy:41.476,test_accuracy:39.97\n",
            "Epoch7,train_accuracy:43.652,test_accuracy:45.05\n",
            "Epoch8,train_accuracy:45.69,test_accuracy:47.07\n",
            "Epoch9,train_accuracy:47.884,test_accuracy:47.62\n",
            "Epoch10,train_accuracy:49.026,test_accuracy:49.32\n",
            "Epoch11,train_accuracy:50.158,test_accuracy:50.56\n",
            "Epoch12,train_accuracy:51.306,test_accuracy:50.25\n",
            "Epoch13,train_accuracy:52.104,test_accuracy:51.94\n",
            "Epoch14,train_accuracy:52.944,test_accuracy:52.51\n",
            "Epoch15,train_accuracy:53.678,test_accuracy:51.3\n",
            "Epoch16,train_accuracy:54.214,test_accuracy:53.07\n",
            "Epoch17,train_accuracy:55.3,test_accuracy:52.81\n",
            "Epoch18,train_accuracy:55.654,test_accuracy:55.49\n",
            "Epoch19,train_accuracy:56.254,test_accuracy:54.96\n",
            "Epoch20,train_accuracy:56.726,test_accuracy:54.88\n",
            "Epoch21,train_accuracy:57.004,test_accuracy:56.74\n",
            "Epoch22,train_accuracy:57.578,test_accuracy:55.96\n",
            "Epoch23,train_accuracy:58.022,test_accuracy:54.46\n",
            "Epoch24,train_accuracy:58.398,test_accuracy:55.74\n",
            "Epoch25,train_accuracy:58.374,test_accuracy:56.83\n",
            "Epoch26,train_accuracy:58.936,test_accuracy:56.38\n",
            "Epoch27,train_accuracy:59.294,test_accuracy:57.79\n",
            "Epoch28,train_accuracy:59.49,test_accuracy:58.21\n",
            "Epoch29,train_accuracy:59.876,test_accuracy:57.72\n",
            "Epoch30,train_accuracy:59.922,test_accuracy:57.69\n",
            "Epoch31,train_accuracy:60.09,test_accuracy:58.31\n",
            "Epoch32,train_accuracy:60.45,test_accuracy:58.29\n",
            "Epoch33,train_accuracy:60.84,test_accuracy:57.81\n",
            "Epoch34,train_accuracy:60.824,test_accuracy:58.51\n",
            "Epoch35,train_accuracy:61.318,test_accuracy:58.23\n",
            "Epoch36,train_accuracy:61.368,test_accuracy:59.11\n",
            "Epoch37,train_accuracy:61.614,test_accuracy:58.64\n",
            "Epoch38,train_accuracy:61.692,test_accuracy:59.46\n",
            "Epoch39,train_accuracy:61.888,test_accuracy:59.92\n",
            "Epoch40,train_accuracy:62.042,test_accuracy:58.62\n",
            "Epoch41,train_accuracy:62.448,test_accuracy:59.06\n",
            "Epoch42,train_accuracy:62.416,test_accuracy:59.53\n",
            "Epoch43,train_accuracy:62.69,test_accuracy:59.81\n",
            "Epoch44,train_accuracy:62.782,test_accuracy:59.08\n",
            "Epoch45,train_accuracy:62.96,test_accuracy:59.23\n",
            "Epoch46,train_accuracy:63.0,test_accuracy:58.98\n",
            "Epoch47,train_accuracy:63.094,test_accuracy:59.53\n",
            "Epoch48,train_accuracy:63.31,test_accuracy:60.02\n",
            "Epoch49,train_accuracy:63.48,test_accuracy:59.64\n"
          ]
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== All Results ===\")\n",
        "for result in results:\n",
        "    print(f\"Optimizer: {result['optimizer']}, \"\n",
        "          f\"Learning Rate: {result['learning_rate']}, \"\n",
        "          f\"Train Accuracy: {result['train_acc']:.2f}%, \"\n",
        "          f\"Test Accuracy: {result['test_acc']:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2LC44YGqaSP",
        "outputId": "500c4b2a-7b54-402a-94b5-aeefea880f4e"
      },
      "id": "C2LC44YGqaSP",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== All Results ===\n",
            "Optimizer: SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    differentiable: False\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.005\n",
            "    maximize: False\n",
            "    momentum: 0.9\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            "), Learning Rate: 0.005, Train Accuracy: 97.44%, Test Accuracy: 89.13%\n",
            "Optimizer: SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    differentiable: False\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    momentum: 0.9\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            "), Learning Rate: 0.001, Train Accuracy: 95.23%, Test Accuracy: 86.69%\n",
            "Optimizer: SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    differentiable: False\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.0005\n",
            "    maximize: False\n",
            "    momentum: 0.9\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            "), Learning Rate: 0.0005, Train Accuracy: 63.48%, Test Accuracy: 59.64%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}