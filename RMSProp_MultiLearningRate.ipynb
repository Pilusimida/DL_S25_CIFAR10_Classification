{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!pip install -q kaggle\n",
        "!kaggle --version\n",
        "!kaggle competitions download -c deep-learning-spring-2025-project-1\n",
        "!unzip -q deep-learning-spring-2025-project-1.zip -d ./data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "id": "rIR2vKla6pRW",
        "outputId": "a0422a66-acba-4fdb-c156-9eb2fd987d1d"
      },
      "id": "rIR2vKla6pRW",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-66601b12-aa81-4966-9655-e93e38ee6af4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-66601b12-aa81-4966-9655-e93e38ee6af4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Kaggle API 1.6.17\n",
            "Downloading deep-learning-spring-2025-project-1.zip to /content\n",
            "100% 189M/189M [00:09<00:00, 21.9MB/s]\n",
            "100% 189M/189M [00:09<00:00, 20.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "847113d0cd4034e6"
      },
      "cell_type": "markdown",
      "source": [
        "1. Import dependency"
      ],
      "id": "847113d0cd4034e6"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-13T07:20:56.823126Z",
          "start_time": "2025-03-13T07:20:56.819692Z"
        },
        "id": "efb1f80b5f47bedd"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from data_handler import DataHandler\n",
        "import numpy as np\n",
        "import torch.nn.functional as F"
      ],
      "id": "efb1f80b5f47bedd",
      "outputs": [],
      "execution_count": 2
    },
    {
      "metadata": {
        "id": "2cd4077e329f3fed"
      },
      "cell_type": "markdown",
      "source": [
        " 2. Dataset"
      ],
      "id": "2cd4077e329f3fed"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-13T07:20:57.698109Z",
          "start_time": "2025-03-13T07:20:57.694306Z"
        },
        "id": "8aa1c00c212a35a"
      },
      "cell_type": "code",
      "source": [
        "# 数据集类\n",
        "class CIFAR10Dataset(Dataset):\n",
        "    def __init__(self, data_handler, files, transform=None):\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        self.transform = transform\n",
        "\n",
        "        for file in files:\n",
        "            batch = data_handler.unpickle(file)\n",
        "            images = batch[b\"data\"].reshape(-1, 3, 32, 32).astype(np.uint8)\n",
        "            labels = batch[b\"labels\"]\n",
        "            self.data.append(images)\n",
        "            self.labels.extend(labels)\n",
        "\n",
        "        self.data = np.vstack(self.data)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.data[idx]\n",
        "        image = np.transpose(image, (1, 2, 0))\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n"
      ],
      "id": "8aa1c00c212a35a",
      "outputs": [],
      "execution_count": 3
    },
    {
      "metadata": {
        "id": "7e7b466896e08482"
      },
      "cell_type": "markdown",
      "source": [
        "3. Data Process"
      ],
      "id": "7e7b466896e08482"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-13T07:20:59.029565Z",
          "start_time": "2025-03-13T07:20:58.718722Z"
        },
        "id": "be0c5fe06f320626"
      },
      "cell_type": "code",
      "source": [
        "# 数据处理\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "])\n",
        "\n",
        "data_handler = DataHandler(\"./data/cifar-10-python/cifar-10-batches-py/\")\n",
        "train_files = [f\"data_batch_{i}\" for i in range(1, 6)]\n",
        "test_files = [\"test_batch\"]\n",
        "\n",
        "train_dataset = CIFAR10Dataset(data_handler, train_files, transform=transform)\n",
        "test_dataset = CIFAR10Dataset(data_handler, test_files, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n"
      ],
      "id": "be0c5fe06f320626",
      "outputs": [],
      "execution_count": 4
    },
    {
      "metadata": {
        "id": "efbdf0eb1c0dd9d8"
      },
      "cell_type": "markdown",
      "source": [
        "4. Build Model"
      ],
      "id": "efbdf0eb1c0dd9d8"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-13T07:20:59.808676Z",
          "start_time": "2025-03-13T07:20:59.802425Z"
        },
        "id": "c8bde7ad18aa3c0b"
      },
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, stride=1):\n",
        "    super(ResidualBlock, self).__init__()\n",
        "    # Using 3x3 square kernel, padding=1 ensure that the output size is the same as the input size.\n",
        "    self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=True)\n",
        "    self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True)\n",
        "    self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    self.sample = None\n",
        "    if stride != 1 or in_channels != out_channels:\n",
        "      self.sample = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=True),\n",
        "        nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "  def forward(self, x):\n",
        "    residualx = x\n",
        "    if self.sample is not None:\n",
        "      residualx = self.sample(x)\n",
        "\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = nn.ReLU(inplace=True)(out)\n",
        "\n",
        "    out = self.conv2(out)\n",
        "    out = self.bn2(out)\n",
        "    out += residualx\n",
        "    out = nn.ReLU(inplace=True)(out)\n",
        "\n",
        "    return out"
      ],
      "id": "c8bde7ad18aa3c0b",
      "outputs": [],
      "execution_count": 5
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-13T08:00:09.186720Z",
          "start_time": "2025-03-13T08:00:09.176693Z"
        },
        "id": "6b6de078e776ac27"
      },
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "  def __init__(self, block, layers, num_classes = 10):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.in_channels = 64\n",
        "    self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(64)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    self.layer1 = self._make_layer(block, 50, layers[0])\n",
        "    self.layer2 = self._make_layer(block, 100, layers[1], stride=2)\n",
        "    self.layer3 = self._make_layer(block, 200, layers[2], stride=2)\n",
        "    # self.layer4 = self._make_layer(block, 336, layers[3], stride=2)\n",
        "    self.layer4 = self._make_layer(block, 200, layers[3], stride=2)\n",
        "    self.layer5 = self._make_layer(block, 200, layers[3], stride=2)\n",
        "\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "    # self.fc = nn.Linear(336, num_classes)\n",
        "    self.fc_1 = nn.Linear(200, 100)\n",
        "    self.fc_2 = nn.Linear(100, 50)\n",
        "    self.fc_3 = nn.Linear(50, num_classes)\n",
        "\n",
        "\n",
        "  def _make_layer(self, block, out_channels, num_blocks, stride=1):\n",
        "    strides = [stride] + [1] * (num_blocks - 1)\n",
        "    layers = []\n",
        "    for stride in strides:\n",
        "      layers.append(block(self.in_channels, out_channels, stride))\n",
        "      self.in_channels = out_channels\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    # x = self.maxpool(x)\n",
        "\n",
        "    x = self.layer1(x)\n",
        "    x = self.layer2(x)\n",
        "    x = self.layer3(x)\n",
        "    x = self.layer4(x)\n",
        "\n",
        "    x = self.layer5(x)\n",
        "\n",
        "    x = self.avgpool(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    # x = self.fc(x)\n",
        "    x = self.relu(self.fc_1(x))\n",
        "    x = self.relu(self.fc_2(x))\n",
        "    x = self.relu(self.fc_3(x))\n",
        "    x = F.log_softmax(x, dim=1)\n",
        "\n",
        "    return x"
      ],
      "id": "6b6de078e776ac27",
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(model, train_loader, test_loader, optimizer, criterion, num_epochs=10):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "            total_train += labels.size(0)\n",
        "\n",
        "        train_accuracy = 100 * correct_train / total_train\n",
        "\n",
        "        # === 测试阶段 ===\n",
        "        model.eval()\n",
        "        correct_test = 0\n",
        "        total_test = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                correct_test += (predicted == labels).sum().item()\n",
        "                total_test += labels.size(0)\n",
        "\n",
        "        test_accuracy = 100 * correct_test / total_test\n",
        "        print(f\"Epoch{epoch},train_accuracy:{train_accuracy},test_accuracy:{test_accuracy}\")\n",
        "    return train_accuracy, test_accuracy"
      ],
      "metadata": {
        "id": "ox-PP8s4oGMc"
      },
      "id": "ox-PP8s4oGMc",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layers = [2, 2, 2, 2]\n",
        "model = ResNet(ResidualBlock, layers).cuda()\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "from torchsummary import summary\n",
        "summary(model, input_size=(3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMrHyCinplMp",
        "outputId": "a920a654-501e-4bc1-8fbc-b150bfcb0211"
      },
      "id": "OMrHyCinplMp",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "              ReLU-3           [-1, 64, 32, 32]               0\n",
            "            Conv2d-4           [-1, 50, 32, 32]           3,250\n",
            "       BatchNorm2d-5           [-1, 50, 32, 32]             100\n",
            "            Conv2d-6           [-1, 50, 32, 32]          28,850\n",
            "       BatchNorm2d-7           [-1, 50, 32, 32]             100\n",
            "            Conv2d-8           [-1, 50, 32, 32]          22,550\n",
            "       BatchNorm2d-9           [-1, 50, 32, 32]             100\n",
            "    ResidualBlock-10           [-1, 50, 32, 32]               0\n",
            "           Conv2d-11           [-1, 50, 32, 32]          22,550\n",
            "      BatchNorm2d-12           [-1, 50, 32, 32]             100\n",
            "           Conv2d-13           [-1, 50, 32, 32]          22,550\n",
            "      BatchNorm2d-14           [-1, 50, 32, 32]             100\n",
            "    ResidualBlock-15           [-1, 50, 32, 32]               0\n",
            "           Conv2d-16          [-1, 100, 16, 16]           5,100\n",
            "      BatchNorm2d-17          [-1, 100, 16, 16]             200\n",
            "           Conv2d-18          [-1, 100, 16, 16]          45,100\n",
            "      BatchNorm2d-19          [-1, 100, 16, 16]             200\n",
            "           Conv2d-20          [-1, 100, 16, 16]          90,100\n",
            "      BatchNorm2d-21          [-1, 100, 16, 16]             200\n",
            "    ResidualBlock-22          [-1, 100, 16, 16]               0\n",
            "           Conv2d-23          [-1, 100, 16, 16]          90,100\n",
            "      BatchNorm2d-24          [-1, 100, 16, 16]             200\n",
            "           Conv2d-25          [-1, 100, 16, 16]          90,100\n",
            "      BatchNorm2d-26          [-1, 100, 16, 16]             200\n",
            "    ResidualBlock-27          [-1, 100, 16, 16]               0\n",
            "           Conv2d-28            [-1, 200, 8, 8]          20,200\n",
            "      BatchNorm2d-29            [-1, 200, 8, 8]             400\n",
            "           Conv2d-30            [-1, 200, 8, 8]         180,200\n",
            "      BatchNorm2d-31            [-1, 200, 8, 8]             400\n",
            "           Conv2d-32            [-1, 200, 8, 8]         360,200\n",
            "      BatchNorm2d-33            [-1, 200, 8, 8]             400\n",
            "    ResidualBlock-34            [-1, 200, 8, 8]               0\n",
            "           Conv2d-35            [-1, 200, 8, 8]         360,200\n",
            "      BatchNorm2d-36            [-1, 200, 8, 8]             400\n",
            "           Conv2d-37            [-1, 200, 8, 8]         360,200\n",
            "      BatchNorm2d-38            [-1, 200, 8, 8]             400\n",
            "    ResidualBlock-39            [-1, 200, 8, 8]               0\n",
            "           Conv2d-40            [-1, 200, 4, 4]          40,200\n",
            "      BatchNorm2d-41            [-1, 200, 4, 4]             400\n",
            "           Conv2d-42            [-1, 200, 4, 4]         360,200\n",
            "      BatchNorm2d-43            [-1, 200, 4, 4]             400\n",
            "           Conv2d-44            [-1, 200, 4, 4]         360,200\n",
            "      BatchNorm2d-45            [-1, 200, 4, 4]             400\n",
            "    ResidualBlock-46            [-1, 200, 4, 4]               0\n",
            "           Conv2d-47            [-1, 200, 4, 4]         360,200\n",
            "      BatchNorm2d-48            [-1, 200, 4, 4]             400\n",
            "           Conv2d-49            [-1, 200, 4, 4]         360,200\n",
            "      BatchNorm2d-50            [-1, 200, 4, 4]             400\n",
            "    ResidualBlock-51            [-1, 200, 4, 4]               0\n",
            "           Conv2d-52            [-1, 200, 2, 2]          40,200\n",
            "      BatchNorm2d-53            [-1, 200, 2, 2]             400\n",
            "           Conv2d-54            [-1, 200, 2, 2]         360,200\n",
            "      BatchNorm2d-55            [-1, 200, 2, 2]             400\n",
            "           Conv2d-56            [-1, 200, 2, 2]         360,200\n",
            "      BatchNorm2d-57            [-1, 200, 2, 2]             400\n",
            "    ResidualBlock-58            [-1, 200, 2, 2]               0\n",
            "           Conv2d-59            [-1, 200, 2, 2]         360,200\n",
            "      BatchNorm2d-60            [-1, 200, 2, 2]             400\n",
            "           Conv2d-61            [-1, 200, 2, 2]         360,200\n",
            "      BatchNorm2d-62            [-1, 200, 2, 2]             400\n",
            "    ResidualBlock-63            [-1, 200, 2, 2]               0\n",
            "AdaptiveAvgPool2d-64            [-1, 200, 1, 1]               0\n",
            "           Linear-65                  [-1, 100]          20,100\n",
            "             ReLU-66                  [-1, 100]               0\n",
            "           Linear-67                   [-1, 50]           5,050\n",
            "             ReLU-68                   [-1, 50]               0\n",
            "           Linear-69                   [-1, 10]             510\n",
            "             ReLU-70                   [-1, 10]               0\n",
            "================================================================\n",
            "Total params: 4,698,266\n",
            "Trainable params: 4,698,266\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 10.07\n",
            "Params size (MB): 17.92\n",
            "Estimated Total Size (MB): 28.01\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-13T08:00:11.148840Z",
          "start_time": "2025-03-13T08:00:11.106453Z"
        },
        "id": "d00674217f036b38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4066ca9-aa35-41f1-9827-ceeecbdbcafb"
      },
      "cell_type": "code",
      "source": [
        "learning_rates = [0.005, 0.001, 0.0005]\n",
        "results = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    model = ResNet(ResidualBlock, layers).cuda()\n",
        "    optimizer = optim.RMSprop(model.parameters(), lr=lr, alpha = 0.9)\n",
        "\n",
        "    print(f\"\\nTraining with RMSprop (lr={lr})...\")\n",
        "    train_acc, test_acc = train_and_evaluate(model, train_loader, test_loader, optimizer, criterion, 50)\n",
        "\n",
        "    results.append({\n",
        "        'optimizer': optimizer,\n",
        "        'learning_rate': lr,\n",
        "        'train_acc': train_acc,\n",
        "        'test_acc': test_acc\n",
        "    })"
      ],
      "id": "d00674217f036b38",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with RMSprop (lr=0.005)...\n",
            "Epoch0,train_accuracy:19.816,test_accuracy:18.77\n",
            "Epoch1,train_accuracy:30.54,test_accuracy:37.33\n",
            "Epoch2,train_accuracy:42.902,test_accuracy:41.28\n",
            "Epoch3,train_accuracy:52.032,test_accuracy:47.35\n",
            "Epoch4,train_accuracy:60.194,test_accuracy:57.65\n",
            "Epoch5,train_accuracy:64.064,test_accuracy:64.5\n",
            "Epoch6,train_accuracy:67.226,test_accuracy:61.4\n",
            "Epoch7,train_accuracy:69.208,test_accuracy:66.09\n",
            "Epoch8,train_accuracy:71.336,test_accuracy:67.14\n",
            "Epoch9,train_accuracy:72.78,test_accuracy:65.05\n",
            "Epoch10,train_accuracy:73.796,test_accuracy:69.32\n",
            "Epoch11,train_accuracy:74.974,test_accuracy:66.8\n",
            "Epoch12,train_accuracy:76.036,test_accuracy:70.21\n",
            "Epoch13,train_accuracy:76.732,test_accuracy:73.59\n",
            "Epoch14,train_accuracy:77.238,test_accuracy:74.72\n",
            "Epoch15,train_accuracy:77.738,test_accuracy:73.59\n",
            "Epoch16,train_accuracy:78.532,test_accuracy:74.41\n",
            "Epoch17,train_accuracy:78.908,test_accuracy:76.06\n",
            "Epoch18,train_accuracy:79.332,test_accuracy:75.69\n",
            "Epoch19,train_accuracy:79.272,test_accuracy:76.38\n",
            "Epoch20,train_accuracy:79.994,test_accuracy:76.2\n",
            "Epoch21,train_accuracy:80.196,test_accuracy:72.99\n",
            "Epoch22,train_accuracy:80.652,test_accuracy:75.84\n",
            "Epoch23,train_accuracy:81.062,test_accuracy:76.82\n",
            "Epoch24,train_accuracy:81.188,test_accuracy:77.61\n",
            "Epoch25,train_accuracy:81.38,test_accuracy:78.3\n",
            "Epoch26,train_accuracy:81.538,test_accuracy:78.74\n",
            "Epoch27,train_accuracy:81.556,test_accuracy:79.32\n",
            "Epoch28,train_accuracy:81.874,test_accuracy:76.93\n",
            "Epoch29,train_accuracy:82.026,test_accuracy:75.34\n",
            "Epoch30,train_accuracy:81.924,test_accuracy:77.0\n",
            "Epoch31,train_accuracy:82.254,test_accuracy:77.91\n",
            "Epoch32,train_accuracy:82.378,test_accuracy:77.38\n",
            "Epoch33,train_accuracy:82.28,test_accuracy:78.5\n",
            "Epoch34,train_accuracy:82.73,test_accuracy:79.51\n",
            "Epoch35,train_accuracy:82.656,test_accuracy:77.91\n",
            "Epoch36,train_accuracy:82.838,test_accuracy:76.74\n",
            "Epoch37,train_accuracy:83.098,test_accuracy:77.81\n",
            "Epoch38,train_accuracy:83.222,test_accuracy:78.23\n",
            "Epoch39,train_accuracy:83.262,test_accuracy:79.32\n",
            "Epoch40,train_accuracy:83.29,test_accuracy:78.03\n",
            "Epoch41,train_accuracy:83.362,test_accuracy:77.97\n",
            "Epoch42,train_accuracy:83.392,test_accuracy:79.37\n",
            "Epoch43,train_accuracy:83.632,test_accuracy:79.19\n",
            "Epoch44,train_accuracy:83.068,test_accuracy:76.17\n",
            "Epoch45,train_accuracy:83.286,test_accuracy:79.52\n",
            "Epoch46,train_accuracy:83.506,test_accuracy:77.22\n",
            "Epoch47,train_accuracy:83.83,test_accuracy:78.62\n",
            "Epoch48,train_accuracy:83.67,test_accuracy:79.5\n",
            "Epoch49,train_accuracy:83.856,test_accuracy:78.57\n",
            "\n",
            "Training with RMSprop (lr=0.001)...\n",
            "Epoch0,train_accuracy:24.766,test_accuracy:29.66\n",
            "Epoch1,train_accuracy:45.968,test_accuracy:48.25\n",
            "Epoch2,train_accuracy:53.856,test_accuracy:54.46\n",
            "Epoch3,train_accuracy:57.812,test_accuracy:53.21\n",
            "Epoch4,train_accuracy:60.734,test_accuracy:53.83\n",
            "Epoch5,train_accuracy:63.284,test_accuracy:54.81\n",
            "Epoch6,train_accuracy:65.202,test_accuracy:62.03\n",
            "Epoch7,train_accuracy:66.792,test_accuracy:64.46\n",
            "Epoch8,train_accuracy:68.184,test_accuracy:67.01\n",
            "Epoch9,train_accuracy:69.098,test_accuracy:65.61\n",
            "Epoch10,train_accuracy:70.03,test_accuracy:64.82\n",
            "Epoch11,train_accuracy:70.842,test_accuracy:67.95\n",
            "Epoch12,train_accuracy:71.15,test_accuracy:70.23\n",
            "Epoch13,train_accuracy:71.818,test_accuracy:71.11\n",
            "Epoch14,train_accuracy:72.434,test_accuracy:69.7\n",
            "Epoch15,train_accuracy:72.768,test_accuracy:70.74\n",
            "Epoch16,train_accuracy:73.216,test_accuracy:70.14\n",
            "Epoch17,train_accuracy:73.542,test_accuracy:71.56\n",
            "Epoch18,train_accuracy:73.956,test_accuracy:70.7\n",
            "Epoch19,train_accuracy:74.014,test_accuracy:70.77\n",
            "Epoch20,train_accuracy:74.408,test_accuracy:71.48\n",
            "Epoch21,train_accuracy:74.638,test_accuracy:70.3\n",
            "Epoch22,train_accuracy:74.938,test_accuracy:70.05\n",
            "Epoch23,train_accuracy:74.902,test_accuracy:69.83\n",
            "Epoch24,train_accuracy:75.206,test_accuracy:71.92\n",
            "Epoch25,train_accuracy:75.336,test_accuracy:72.17\n",
            "Epoch26,train_accuracy:75.536,test_accuracy:71.83\n",
            "Epoch27,train_accuracy:75.938,test_accuracy:71.69\n",
            "Epoch28,train_accuracy:76.012,test_accuracy:72.49\n",
            "Epoch29,train_accuracy:76.072,test_accuracy:72.1\n",
            "Epoch30,train_accuracy:76.148,test_accuracy:72.73\n",
            "Epoch31,train_accuracy:76.41,test_accuracy:72.91\n",
            "Epoch32,train_accuracy:76.4,test_accuracy:72.22\n",
            "Epoch33,train_accuracy:76.52,test_accuracy:73.8\n",
            "Epoch34,train_accuracy:76.66,test_accuracy:72.83\n",
            "Epoch35,train_accuracy:76.644,test_accuracy:71.97\n",
            "Epoch36,train_accuracy:76.766,test_accuracy:73.03\n",
            "Epoch37,train_accuracy:76.768,test_accuracy:73.01\n",
            "Epoch38,train_accuracy:76.972,test_accuracy:73.1\n",
            "Epoch39,train_accuracy:77.062,test_accuracy:72.46\n",
            "Epoch40,train_accuracy:77.234,test_accuracy:72.71\n",
            "Epoch41,train_accuracy:77.188,test_accuracy:73.6\n",
            "Epoch42,train_accuracy:77.36,test_accuracy:71.68\n",
            "Epoch43,train_accuracy:77.454,test_accuracy:72.44\n",
            "Epoch44,train_accuracy:77.316,test_accuracy:71.95\n",
            "Epoch45,train_accuracy:77.5,test_accuracy:72.69\n",
            "Epoch46,train_accuracy:77.49,test_accuracy:72.9\n",
            "Epoch47,train_accuracy:77.484,test_accuracy:73.35\n",
            "Epoch48,train_accuracy:77.58,test_accuracy:73.68\n",
            "Epoch49,train_accuracy:77.668,test_accuracy:73.2\n",
            "\n",
            "Training with RMSprop (lr=0.0005)...\n",
            "Epoch0,train_accuracy:33.326,test_accuracy:39.61\n",
            "Epoch1,train_accuracy:46.678,test_accuracy:40.99\n",
            "Epoch2,train_accuracy:52.088,test_accuracy:46.89\n",
            "Epoch3,train_accuracy:55.504,test_accuracy:52.81\n",
            "Epoch4,train_accuracy:57.508,test_accuracy:53.0\n",
            "Epoch5,train_accuracy:58.854,test_accuracy:58.1\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== All Results ===\")\n",
        "for result in results:\n",
        "    print(f\"Optimizer: {result['optimizer']}, \"\n",
        "          f\"Learning Rate: {result['learning_rate']}, \"\n",
        "          f\"Train Accuracy: {result['train_acc']:.2f}%, \"\n",
        "          f\"Test Accuracy: {result['test_acc']:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2LC44YGqaSP",
        "outputId": "b01d086c-6ead-4e01-a5b3-9e996cd67b09"
      },
      "id": "C2LC44YGqaSP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== All Results ===\n",
            "Optimizer: SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    differentiable: False\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.005\n",
            "    maximize: False\n",
            "    momentum: 0.9\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            "), Learning Rate: 0.005, Train Accuracy: 30.96%, Test Accuracy: 40.97%\n",
            "Optimizer: SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    differentiable: False\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    momentum: 0.9\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            "), Learning Rate: 0.001, Train Accuracy: 18.68%, Test Accuracy: 25.34%\n",
            "Optimizer: SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    differentiable: False\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.0005\n",
            "    maximize: False\n",
            "    momentum: 0.9\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            "), Learning Rate: 0.0005, Train Accuracy: 15.76%, Test Accuracy: 21.33%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}